{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration using beautiful soup\n",
    "## Finding companies advertising scientist jobs in Houston\n",
    "- based on this: https://jessesw.com/Data-Science-Skills/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # For HTML parsing\n",
    "# import urllib2 # Website connections\n",
    "from urllib.request import urlopen\n",
    "import re # Regular expressions\n",
    "from time import sleep # To prevent overwhelming the server between connections\n",
    "from collections import Counter # Keep track of our term counts\n",
    "from nltk.corpus import stopwords # Filter out stopwords, such as 'the', 'or', 'and'\n",
    "import pandas as pd # For converting results to a dataframe and bar chart plots\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def skills_info(city=\"Houston\", state=\"TX\",job_title='data+scientist'):\n",
    "    '''\n",
    "    This function will take a desired city/state and look for all new job postings\n",
    "    on Indeed.com. It will crawl all of the job postings and keep track of how many\n",
    "    use a preset list of typical data science skills. The final percentage for each skill\n",
    "    is then displayed at the end of the collation. \n",
    "        \n",
    "    Inputs: The location's city and state. These are optional. If no city/state is input, \n",
    "    the function will assume a national search (this can take a while!!!).\n",
    "    Input the city/state as strings, such as skills_info('Chicago', 'IL').\n",
    "    Use a two letter abbreviation for the state.\n",
    "    \n",
    "    Output: A bar chart showing the most commonly desired skills in the job market for \n",
    "    a data scientist. \n",
    "    '''\n",
    "    #print(\"test1\")   \n",
    "    final_job = job_title # searching for data scientist exact fit(\"data scientist\" on Indeed search)\n",
    "    \n",
    "    # Make sure the city specified works properly if it has more than one word (such as San Francisco)  %22&l=\n",
    "    if city is not None:\n",
    "        final_city = city.split() \n",
    "        final_city = '+'.join(word for word in final_city)\n",
    "        final_site_list = ['https://www.indeed.com/jobs?q=', final_job, '&l=', final_city,'%2C+', state] # Join all of our strings together so that indeed will search correctly\n",
    "    else:\n",
    "        final_site_list = ['https://www.indeed.com/jobs?q=\"', final_job, '\"']\n",
    "\n",
    "    final_site = ''.join(final_site_list) # Merge the html address together into one string\n",
    "\n",
    "    # website query today (2017-02-25) is = https://www.indeed.com/jobs?q=data+scientist&l=Houston%2CTX\n",
    "        \n",
    "    base_url = 'http://www.indeed.com'\n",
    "    \n",
    "    #print(\"test2\")  \n",
    "    try:\n",
    "        html = urlopen(final_site).read() # Open up the front page of our search first\n",
    "    except:\n",
    "        #print('That city/state combination did not have any jobs. Exiting . . .') # In case the city is invalid\n",
    "        return\n",
    "    soup = BeautifulSoup(html,\"lxml\") # Get the html from the first page\n",
    "    \n",
    "    # Now find out how many jobs there were\n",
    "    \n",
    "    num_jobs_area = soup.find(id = 'searchCount').string.encode('utf-8') # Now extract the total number of jobs found\n",
    "                                                                        # The 'searchCount' object has this\n",
    "    #print('num_jobs_area before filtering ', num_jobs_area)\n",
    "    \n",
    "    job_numbers = re.findall(b'\\d+', num_jobs_area) # Extract the total jobs found from the search result\n",
    "     \n",
    "    #print('job number before filtering ', job_numbers[2])\n",
    "    \n",
    "    job_numbers_decoded = int(job_numbers[2].decode('utf-8'))\n",
    "    #print('job_numbers_decoded = ',job_numbers_decoded, \"type is \",type(job_numbers_decoded))\n",
    "    \n",
    "    #if len(job_numbers[2]) > 3: # Have a total number of jobs greater than 1000\n",
    "    #    total_num_jobs = (int(job_numbers[2])*1000) + int(job_numbers[3])\n",
    "    #else:\n",
    "    #    total_num_jobs = int(job_numbers[2]) \n",
    "    \n",
    "    total_num_jobs = job_numbers_decoded\n",
    "    \n",
    "    city_title = city\n",
    "    if city is None:\n",
    "        city_title = 'Nationwide'\n",
    "        \n",
    "    #print('There were', total_num_jobs, 'jobs found,', city_title) # Display how many jobs were found\n",
    "    \n",
    "    # took out /10 here, which seemed to be assuming always 10 results per page? now 15?\n",
    "    num_pages = int(float(total_num_jobs/10)) # This will be how we know the number of times we need to iterate over each new\n",
    "                                      # search result page\n",
    "        \n",
    "    job_descriptions = [] # Store all our descriptions in this list\n",
    "    \n",
    "    # used to be 'xrange' here but I changed it to 'range' because I'm using python3 and xrange is python2 only\n",
    "    all_company_results = []\n",
    "    for i in range(1,num_pages+1): # Loop through all of our search result pages\n",
    "        #print('Getting page', i)\n",
    "        \n",
    "        start_num = str(i*10) # Assign the multiplier of 10 to view the pages we want\n",
    "        current_page = ''.join([final_site, '&start=', start_num])\n",
    "        # Now that we can view the correct 10 job returns, start collecting the text samples from each\n",
    "            \n",
    "        html_page = urlopen(current_page).read() # Get the page\n",
    "            \n",
    "        page_obj = BeautifulSoup(html_page,\"lxml\") # Locate all of the job links\n",
    "        job_link_area = page_obj.find(id = 'resultsCol') # The center column on the page where the job postings exist\n",
    "        #print('page_obj',page_obj)  \n",
    "            \n",
    "        #print(\"link.get('href')\",base_url + link.get('href')\n",
    "        #print(\"job_link_area.find_all('a')\",job_link_area.find_all('a'))\n",
    "        \n",
    "        for a in job_link_area.find_all('a', href=True):\n",
    "            #print (\"Found the URL:\", a['href'])\n",
    "            if \"/cmp/\" in a['href']: \n",
    "                #  if a['href'].find(\"/cmp/\"):\n",
    "                #print(\"a had /cmp/ in it using find = \",a['href'])\n",
    "                arrayHrefParts_0 = a['href'].split('/cmp/')\n",
    "                #print(\"aarrayHrefParts_0 = \",arrayHrefParts_0)\n",
    "                arrayHrefParts_1 = arrayHrefParts_0[1].split('/')\n",
    "                #print(\"arrayHrefParts_1 = \",arrayHrefParts_1)\n",
    "                company = arrayHrefParts_1[0]\n",
    "                #print('company = ',company)\n",
    "                all_company_results.append(company)\n",
    "                #print('all_company_results =',all_company_results)\n",
    "            else:\n",
    "                continue\n",
    "                #print(\"/cmp/ not found in ,\",a['href'])\n",
    "        \n",
    "        #print(\"[base_url + link.get('href') for link in job_link_area.find_all('a')]\",[base_url + link.get('href') for link in job_link_area.find_all('a')])\n",
    "        \n",
    "        #job_URLS = [base_url + link.get('href') for link in job_link_area.find_all('a')] # Get the URLS for the jobs\n",
    "            \n",
    "        #job_URLS = filter(lambda x:'clk' in x, job_URLS) # Now get just the job related URLS\n",
    "            \n",
    "        \n",
    "        #         for j in xrange(0,len(job_URLS)):\n",
    "        #             final_description = text_cleaner(job_URLS[j])\n",
    "        #             if final_description: # So that we only append when the website was accessed correctly\n",
    "        #                 job_descriptions.append(final_description)\n",
    "        #             sleep(1) # So that we don't be jerks. If you have a very fast internet connection you could hit the server a lot! \n",
    "    \n",
    "        sleep(1) # So that we don't be jerks. If you have a very fast internet connection you could hit the server a lot! \n",
    "    \n",
    "    for each in range(len(all_company_results)):\n",
    "        if '.html' in all_company_results[each]:\n",
    "            #print('all_company_results[each]',all_company_results[each],\" each = \",each)\n",
    "            new_name_wo_html = all_company_results[each].split('.html')[0]\n",
    "            #print('new_name_wo_html',new_name_wo_html)\n",
    "            all_company_results[each] = new_name_wo_html  \n",
    "            \n",
    "    all_company_results = list(set(all_company_results))\n",
    "    #print ('Done with collecting the job postings!')    \n",
    "    #print ('all_company_results final = ',all_company_results)\n",
    "    #print (\"There were\",len(all_company_results),\" companies successfully found.\")\n",
    "    return all_company_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test1\n",
      "num_jobs_area before filtering  b'Jobs 1 to 10 of 151'\n",
      "job number before filtering  b'151'\n",
      "job_numbers_decoded =  151 type is  <class 'int'>\n",
      "Done with collecting the job postings!\n",
      "all_company_results final =  ['Engage-Partners', 'Pennwell-Corporation', 'Alliantgroup', 'Genpact-Headstrong-Capital-Markets', 'Texas-Children%27s-Hospital', 'Edf-Trading-North-America-LLC', 'Baylor-College-of-Medicine-%28bcm%29', 'Houston-Rockets', 'Hewlett-Packard-Enterprise', \"Texas-Children's-Hospital\", 'Rolls--royce', 'Wyle-Laboratories', 'Houston-Methodist', 'Bellicum-Pharmaceuticals', 'Baylor-College-of-Medicine-(bcm)', 'Stage-3-Separation-1', 'Occidental-Petroleum', 'Strategic-IT-Staffing', 'Booz-Allen-Hamilton', 'Sysco', 'Hdr', 'Compugra-Systems', 'Exxonmobil', 'Texas-A&M-University', 'Encore-Search-Partners', 'Saudi-Aramco', 'Robert-Half-Technology', 'Texas-A%26M-University', 'Sunnova-Energy-Corporation-1', 'Baylor-College-of-Medicine', 'GE-Corporate', 'Cyberonics', 'Md-Anderson-Cancer-Center', 'Wittaff-Inc.', 'Legacy-Medsearch', 'Texas-Heart-Institute', 'Excell-2', 'Waste-Management', 'Erc,-Inc.', 'Tessella', 'Hart-Energy', 'Expero-Inc', 'Amazon.com', 'Infor', 'Hexion-Inc.', 'Deloitte', 'Noble-Energy', 'Industrial-Defender-1', 'Ut-Health-Science-Center-At-Houston', 'Kprc---Tv-Channel-2', 'The-Aerospace-Corporation', 'Mei-Technologies', 'Erc', 'Arcadis-US-2', 'Invesco', 'Wood-Mackenzie', 'Beckman-Coulter,-Inc.', 'Addison-Group', 'Solvay']\n",
      "There were 59  companies successfully found.\n"
     ]
    }
   ],
   "source": [
    "skills_info_result = skills_info(\"Houston\",\"TX\",'data+scientist')\n",
    "skills_info_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test1\n",
      "num_jobs_area before filtering  b'Jobs 1 to 10 of 41'\n",
      "job number before filtering  b'41'\n",
      "job_numbers_decoded =  41 type is  <class 'int'>\n",
      "Done with collecting the job postings!\n",
      "all_company_results final =  ['Mott-Macdonald', 'Hess-Corporation', 'High-Country-Executive-Search', 'Weatherford', 'W&T-Offshore,-Inc.', 'SAN-Jacinto-College', 'Jab-Recruitment-3', 'Radarview-LLC-1', 'Bmo-Financial-Group', 'Murray-Resources', 'Rbc', 'Targa-Resources', 'Enrud-Resources', 'Sheridan-Production', 'Lee-College', 'Sinopec-Tech-Houston', 'Occidental-Petroleum', 'Oasis-Petroleum', 'Jefferies']\n",
      "There were 19  companies successfully found.\n"
     ]
    }
   ],
   "source": [
    "skills_info_result_geo = skills_info(\"Houston\",\"TX\",'geologist')\n",
    "skills_info_result_geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test1\n",
      "num_jobs_area before filtering  b'Jobs 1 to 10 of 21'\n",
      "job number before filtering  b'21'\n",
      "job_numbers_decoded =  21 type is  <class 'int'>\n",
      "Done with collecting the job postings!\n",
      "all_company_results final =  ['Walker-Elliott', 'Geotrace', 'Hess-Corporation', 'Saudi-Aramco', 'Sinopec-Tech-Houston-LLC', 'Hilcorp-Energy-Company', 'Penn-Virginia-Corporation', 'Murray-Resources', 'Bhp-Billiton', 'BP', 'Petroleum-Geo--services', 'Sinopec-Tech-Houston', 'Confidential---Oil-%26-Gas-Company', 'Occidental-Petroleum']\n",
      "There were 14  companies successfully found.\n"
     ]
    }
   ],
   "source": [
    "skills_info_result_geop = skills_info(\"Houston\",\"TX\",'geophysicist')\n",
    "skills_info_result_geop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runMultiple_skills_info(array_of_jobTitles=['puppies']):\n",
    "    all_results = []\n",
    "    for each in array_of_jobTitles:\n",
    "        print('each= ',each)\n",
    "        #temp_results1 = []\n",
    "        temp_results1 = skills_info(\"Houston\",\"TX\",each)\n",
    "        print('temp_results is ',temp_results1)\n",
    "        #all_results.extend(temp_results1)\n",
    "        all_results.extend(temp_results1)\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "each=  geologist\n",
      "temp_results is  ['Hess-Corporation', 'Rbc', 'Tolunay--wong-Engineers,-Inc.', 'Warren-Averett-Sraffing-and-Recruiting', 'Confidential---Oil-%26-Gas-Company', 'Occidental-Petroleum', 'Mott-Macdonald', 'Hilcorp-Energy-Company', 'Radarview-LLC-1', 'Murray-Resources', 'Targa-Resources', 'Enrud-Resources', 'Sheridan-Production', 'Sinopec-Tech-Houston', 'Lee-College', 'W&T-Offshore,-Inc.', 'Penn-Virginia-Corporation', 'Lonestar-College-System-1', 'Jefferies-&-Company,-Inc.', 'BP', 'New-Tech-Global', 'Jefferies', 'University-of-Houston--downtown', 'High-Country-Executive-Search', 'Weatherford', 'SAN-Jacinto-College', 'Jab-Recruitment-3', 'Bmo-Financial-Group', 'Fugro', 'Oasis-Petroleum']\n",
      "each=  gephysicist\n",
      "temp_results is  ['Walker-Elliott', 'Penn-Virginia-Corporation', 'Sinopec-Tech-Houston-LLC', 'Murray-Resources', 'Sinopec-Tech-Houston', 'Confidential---Oil-%26-Gas-Company']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Hess-Corporation',\n",
       " 'Rbc',\n",
       " 'Tolunay--wong-Engineers,-Inc.',\n",
       " 'Warren-Averett-Sraffing-and-Recruiting',\n",
       " 'Confidential---Oil-%26-Gas-Company',\n",
       " 'Occidental-Petroleum',\n",
       " 'Mott-Macdonald',\n",
       " 'Hilcorp-Energy-Company',\n",
       " 'Radarview-LLC-1',\n",
       " 'Murray-Resources',\n",
       " 'Targa-Resources',\n",
       " 'Enrud-Resources',\n",
       " 'Sheridan-Production',\n",
       " 'Sinopec-Tech-Houston',\n",
       " 'Lee-College',\n",
       " 'W&T-Offshore,-Inc.',\n",
       " 'Penn-Virginia-Corporation',\n",
       " 'Lonestar-College-System-1',\n",
       " 'Jefferies-&-Company,-Inc.',\n",
       " 'BP',\n",
       " 'New-Tech-Global',\n",
       " 'Jefferies',\n",
       " 'University-of-Houston--downtown',\n",
       " 'High-Country-Executive-Search',\n",
       " 'Weatherford',\n",
       " 'SAN-Jacinto-College',\n",
       " 'Jab-Recruitment-3',\n",
       " 'Bmo-Financial-Group',\n",
       " 'Fugro',\n",
       " 'Oasis-Petroleum',\n",
       " 'Walker-Elliott',\n",
       " 'Penn-Virginia-Corporation',\n",
       " 'Sinopec-Tech-Houston-LLC',\n",
       " 'Murray-Resources',\n",
       " 'Sinopec-Tech-Houston',\n",
       " 'Confidential---Oil-%26-Gas-Company']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array2 = ['geologist','gephysicist']\n",
    "geo_and_geo = runMultiple_skills_info(array_of_jobTitles=array2)\n",
    "geo_and_geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
