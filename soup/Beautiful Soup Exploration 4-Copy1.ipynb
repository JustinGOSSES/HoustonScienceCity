{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Notebook With All the Code to Go From List of Science Job Titles to geoJSON of Companies advertising those jobs on a map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Outline\n",
    "### get list of science related jobs\n",
    "    * Clean up a list of science related job titles I modified from wikipedia in excel, save as csv\n",
    "### get company names that are hiring for those science related job titles in Houston, TX by scraping job search site(s)\n",
    "    * use beautiful soup, a python library to gather information from websites programtically and return list of companies\n",
    "    * Clean up duplication and companies that shouldn't be included\n",
    "### get addresses and lat longs associated with each company name in json form\n",
    "    * use google places API to search for company name and a Houston, TX location, returning json of location and company information\n",
    "    * clean returned list of json data such that it is limited to a lat long box of the greater houston area\n",
    "    * clean up duplicates and are false positives useing pandas and other python libraries to \n",
    "### convert results to geoJSON. Add-in additional data dimensions, such as job title used in seach, as geoJSON properties\n",
    "    * use geoJSON and JSON python libraries to convert previous json into points, and then features, and then a single feature collection\n",
    "### get geoJSONs from other sources that contain science location data on things like schools and hospitals\n",
    "    * use .... a http get call to access the geoJSON files already created by the city of Houston.\n",
    "### add geoJSONs to map. Use color, size, and shape to represent different geoJSON properties. Tell a story\n",
    "    * use mapboxGL.js, leaflet.js, and basic html/CSS/JS to create maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing needed python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Imports for parts #1 & #2\n",
    "from bs4 import BeautifulSoup # For HTML parsing\n",
    "from urllib.request import urlopen\n",
    "import re # Regular expressions\n",
    "from time import sleep # To prevent overwhelming the server between connections\n",
    "from collections import Counter # Keep track of our term counts\n",
    "from nltk.corpus import stopwords # Filter out stopwords, such as 'the', 'or', 'and'\n",
    "import pandas as pd # For converting results to a dataframe and bar chart plots\n",
    "%matplotlib inline\n",
    "import json\n",
    "import csv\n",
    "\n",
    "##### Import modules for google API and geoJSON parts. Only duplicate is json\n",
    "import requests\n",
    "import configparser\n",
    "import json\n",
    "import geojson\n",
    "from geojson import Point, Feature, FeatureCollection\n",
    "# https://github.com/frewsxcv/python-geojson\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===================================================================================\n",
    "\n",
    "# 1. Load list of job titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bioanalytical Scientist', 'Biochemist', 'Biochemistry', 'Bioinformatics Research Scientist', 'Biological Engineer', 'Biologist', 'biology  ', 'biomechanical', 'Biomedical scientist', 'biophysics', 'Cell Line', 'Clinical Data', 'Clinical Data Research', 'Clinical Pharmacology Professor', 'Clinical Pharmacy Assistant', 'Clinical Research Associate', 'Drug Evaluator', 'Drug Regulatory Affairs Manager', 'Environmental Health Scientist', 'Genetic Counselor', 'Health Research', 'Health Technology', 'Healthcare science', 'Hospital Research Assistant', 'Immunology Scientist', 'Life Science', 'Medical', 'Medical Center', 'medical Laboratory', 'Medical laboratory scientist', 'Medical Physics Researcher', 'Medical Research Assistant', 'Medical Research Technician', 'Medical Scientist', 'Microbiologist', 'Molecular Biologist', 'Molecular physics', 'Neuroscientist', 'Nuclear', 'Oncology Researcher', 'Pathologist', 'Pharmaceutical Assistant', 'Pharmaceutical Research Analyst', 'Pharmaceutical Research Assistant', 'Pharmaceutical Technician', 'Pharmacy Assistant', 'Photochemistry', 'Public Health Specialist', 'radiobiology', 'Radiochemistry', 'Toxicologist', 'Agricultural', 'Archaeologist', 'Associate Professor', 'Botanist', 'botany', 'Chemical', 'Chemist', 'Climate Data', 'Conservation', 'Field Technician', 'Forensic Chemist', 'Forensic Scientist', 'Herpetologist', 'Laboratory Instructor', 'Laboratory Technician', 'meteorologist ', 'Natural Science', 'Naturalist', 'Oceanographer', 'oceanography', 'optomitrist ', 'Physical Scientist', 'Psychologist', 'Research and Development Associate', 'Research and Development Director', 'Research and Development Manager', 'Research and Development Supervisor', 'Research and Development Technician', 'Research and Innovation', 'Research Fellow', 'Research Scientist', 'Researcher', 'science intern', 'Science teacher', 'Science Writer', 'Scientist', 'social science', 'Solid-state', 'STEM Career Advisor', 'Stem Cell Researcher', 'Stereochemistry', 'Structural Biologist', 'Theoretical', 'Volcanologist', 'volcanology', 'wind power', 'wind turbine ', 'zoo', 'Zoologist', 'zoology ', 'chemistry', 'Crystallography', 'Earth Science', 'Ecologist', 'Ecology', 'Environmental', 'Environmental Emergencies', 'Environmental Research Assistant', 'Environmental Specialist', 'Exploration', 'fluid dynamics', 'geochemist', 'Geochemistry', 'Geographer', 'Geologist', 'geology ', 'geophysicist', 'GiS ', 'Groundwater Technician', 'hydrology', 'Inorganic', 'lab technician', 'Materials science', 'micropaleontologist', 'operations geologist', 'paleoclimatology', 'paleoecology', 'Paleontologist', 'Petroleum geologist', 'petrophysicist', 'playnology', 'Research and Development Chemist', 'Research Chemist', 'reservoir engineer', 'reservoir modeler', 'rock Laboratory', 'sedimentologist', 'stratigraphy', 'subsea engineer', 'aeronautics', 'Aerospace', 'astrobiology', 'Astrochemistry', 'astromaterials', 'Astronaut', 'Astronomer', 'astronomy', 'astrophysics', 'Food chemistry', 'heliophysics', 'International Space Station', 'Johnson Space Center', 'lunar', 'materials engineer', 'NASA', 'Physicist', 'physics', 'Planetary  ', 'planetary geology', 'propulsion engineer', 'Satellite', 'space engineer', 'Space science', 'telemetry']\n"
     ]
    }
   ],
   "source": [
    "with open('../Data/science_jobs_v2.csv', 'rt') as f:\n",
    "    reader = csv.reader(f)\n",
    "    science_jobs_list_withInfo = list(reader)\n",
    "\n",
    "job_title_list = []\n",
    "for each in science_jobs_list_withInfo:\n",
    "    each = each[0]\n",
    "    #print('each',each)\n",
    "    job_title_list.append(each)\n",
    "\n",
    "# take out the header 'Job_Title'\n",
    "job_title_list.pop(0)\n",
    "\n",
    "print(job_title_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "short_job_title_list_1st3rd = ['Biochemistry', 'Bioinformatics', 'Biologist', 'biomechanical', 'Biomedical', 'biophysics', 'Cell+Line', 'Clinical+Data', 'Pharmacology', 'Pharmacy', 'Drug Evaluator', 'Environmental Health', 'Genetic Counselor', 'Healthcare', 'Hospital', 'Immunology', 'Life Science', 'Medical', 'Medical Center', 'medical Laboratory', 'Medical Physics', 'Medical', 'Microbiologist', 'Neuroscientist', 'Nuclear', 'Oncology', 'Pathologist', 'Pharmaceutical', 'Photochemistry', 'Public Health Specialist', 'radiobiology', 'Radiochemistry', 'Toxicologist', 'Agricultural', 'Archaeologist', 'Associate Professor', 'Botanist', 'botany', 'Chemical', 'Chemist', 'Conservation', 'Forensic Chemist', 'Herpetologist', 'Laboratory', 'meteorologist ', 'Naturalist', 'Oceanographer', 'oceanography', 'optomitrist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(short_job_title_list_1st3rd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "short_job_title_list_2nd3rd = ['paleoecology', 'Paleontologist', 'Petroleum geologist', 'petrophysicist', 'playnology', 'reservoir', 'modeler', 'rock Laboratory', 'sedimentologist', 'stratigraphy', 'subsea', 'aeronautics', 'Aerospace', 'astrobiology', 'Astrochemistry', 'astromaterials', 'Astronaut', 'Astronomer', 'astronomy', 'astrophysics', 'Food chemistry', 'heliophysics', 'International Space Station', 'Johnson Space Center', 'lunar', 'materials', 'NASA', 'Physicist', 'physics', 'Planetary', 'propulsion engineer', 'Satellite', 'Space science', 'telemetry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(short_job_title_list_2nd3rd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "short_job_title_list_3rd3rd = ['Forensic Scientist','Physical Scientist','Psychologist', 'Research Fellow', 'Science teacher', 'Scientist', 'social science', 'Solid-state', 'STEM', 'Stem Cell', 'Stereochemistry', 'Volcanologist', 'volcanology', 'wind power', 'wind turbine ', 'zoo', 'Zoologist', 'zoology ', 'chemistry', 'Crystallography', 'Earth Science', 'Ecologist', 'Ecology', 'Environmental', 'Exploration', 'fluid dynamics', 'geochemist', 'Geochemistry', 'Geographer', 'Geologist', 'geology ', 'geophysicist', 'GiS ', 'Groundwater Technician', 'hydrology', 'Inorganic', 'lab technician', 'Materials science', 'micropaleontologist', 'operations geologist', 'paleoclimatology']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(short_job_title_list_3rd3rd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace some of the characters so the search is a bit better:\n",
    "def cleanCompanies(company_array):\n",
    "    new_array = []\n",
    "    for each in company_array:\n",
    "        #print(each, \"and type is \",type(each))\n",
    "        print(each.find(\"-\",0))\n",
    "        print(type(each.find(\"-\",0)),\"type of each.find('-',0\")\n",
    "        #print(type(each.find(\"%2C\",1)))\n",
    "        if each.find(\"%2C\",1) is not -1:\n",
    "            each = each.replace(\"%2C\",\",\")\n",
    "            print(each,\"= each\")\n",
    "        else:\n",
    "            each = each\n",
    "        if each.find(\"%26\",1) is not -1:\n",
    "            each = each.replace(\"%26\",\"&\")\n",
    "            #print(each,\"= each find\")\n",
    "        else:\n",
    "            each = each\n",
    "        ##print(each, \"again with each\")\n",
    "        if  each.find(\"-1\",1) is not -1:\n",
    "            each = each.replace(\"-1\",\"\")\n",
    "        #    print(each,\"= each find\")\n",
    "        else:\n",
    "            each = each\n",
    "        if  each.find(\"-2\",0) is not -1:\n",
    "            each = each.replace(\"-2\",\"\")\n",
    "            print(each,\"= each find\")\n",
    "        else: \n",
    "            each = each\n",
    "        if  each.find(\"-3\",0) is not -1:\n",
    "            each = each.replace(\"-3\",\"\")\n",
    "        #    print(each,\"= each find\")\n",
    "        else:\n",
    "            each\n",
    "        if  each.find(\"-\",1) is not -1:\n",
    "            each = each.replace(\"-\",\"+\")\n",
    "            print(each,\"= each find\")\n",
    "        else:\n",
    "            each = each\n",
    "        if  each.find(\" \",1) is not -1:\n",
    "            each = each.replace(\" \",\"+\")\n",
    "            print(each,\"= each find\")\n",
    "        else:\n",
    "            each = each\n",
    "        print(\"each at end\",each)\n",
    "        new_array.extend([each])\n",
    "    print('new_array',new_array)\n",
    "    return new_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------\n",
    "# 2. Get company names that are hiring for those science related job titles in Houston, TX by scraping job search site(s)\n",
    "------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "=========================================================================================\n",
    "## A data exploration using beautiful soup\n",
    "- based on this: https://jessesw.com/Data-Science-Skills/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The function 'skills_info' takes three arguments; city, state, and job title and returns a list of companies searching for that job title in the last 30 days in Houston, TX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def skills_info(city=\"Houston\", state=\"TX\",job_title='data+scientist'):\n",
    "    '''\n",
    "    This function will take a desired city/state and look for all new job postings\n",
    "    on Indeed.com. It will crawl all of the job postings and keep track of how many\n",
    "    use a preset list of typical data science skills. The final percentage for each skill\n",
    "    is then displayed at the end of the collation. \n",
    "        \n",
    "    Inputs: The location's city and state. These are optional. If no city/state is input, \n",
    "    the function will assume a national search (this can take a while!!!).\n",
    "    Input the city/state as strings, such as skills_info('Chicago', 'IL').\n",
    "    Use a two letter abbreviation for the state.\n",
    "    '''\n",
    "    #print(\"test1\")   \n",
    "    final_job = job_title # searching for data scientist exact fit(\"data scientist\" on Indeed search)\n",
    "    \n",
    "    # Make sure the city specified works properly if it has more than one word (such as San Francisco)  %22&l=\n",
    "    if city is not None:\n",
    "        final_city = city.split() \n",
    "        final_city = '+'.join(word for word in final_city)\n",
    "        final_site_list = ['https://www.indeed.com/jobs?q=', final_job, '&l=', final_city,'%2C+', state] # Join all of our strings together so that indeed will search correctly\n",
    "    else:\n",
    "        final_site_list = ['https://www.indeed.com/jobs?q=\"', final_job, '\"']\n",
    "\n",
    "    final_site = ''.join(final_site_list) # Merge the html address together into one string\n",
    "\n",
    "    # website query today (2017-02-25) is = https://www.indeed.com/jobs?q=data+scientist&l=Houston%2CTX\n",
    "        \n",
    "    base_url = 'http://www.indeed.com'\n",
    "    \n",
    "    #print(\"test2\")  \n",
    "    try:\n",
    "        html = urlopen(final_site).read() # Open up the front page of our search first\n",
    "    except:\n",
    "        #print('That city/state combination did not have any jobs. Exiting . . .') # In case the city is invalid\n",
    "        return\n",
    "    soup = BeautifulSoup(html,\"lxml\") # Get the html from the first page\n",
    "    \n",
    "    # Now find out how many jobs there were\n",
    "    \n",
    "    if type(soup.find(id = 'searchCount')) is None or type(soup.find(id = 'searchCount')) == None:\n",
    "        c\n",
    "    \n",
    "    else:\n",
    "        variableS = soup.find(id = 'searchCount')\n",
    "        if isinstance(variableS,type(None)):\n",
    "            all_company_results = []\n",
    "            return all_company_results\n",
    "        else:\n",
    "            print(\"soup.find(id = 'searchCount') = \",soup.find(id = 'searchCount'))\n",
    "            print(\"soup.find(id = 'searchCount') = \",type(soup.find(id = 'searchCount')))\n",
    "\n",
    "            num_jobs_area = soup.find(id = 'searchCount').string.encode('utf-8') # Now extract the total number of jobs found\n",
    "                                                                                # The 'searchCount' object has this\n",
    "            #print('num_jobs_area before filtering ', num_jobs_area)\n",
    "\n",
    "            job_numbers = re.findall(b'\\d+', num_jobs_area) # Extract the total jobs found from the search result\n",
    "\n",
    "            #print('job number before filtering ', job_numbers[2])\n",
    "\n",
    "            job_numbers_decoded = int(job_numbers[2].decode('utf-8'))\n",
    "            #print('job_numbers_decoded = ',job_numbers_decoded, \"type is \",type(job_numbers_decoded))\n",
    "\n",
    "            #if len(job_numbers[2]) > 3: # Have a total number of jobs greater than 1000\n",
    "            #    total_num_jobs = (int(job_numbers[2])*1000) + int(job_numbers[3])\n",
    "            #else:\n",
    "            #    total_num_jobs = int(job_numbers[2]) \n",
    "\n",
    "            total_num_jobs = job_numbers_decoded\n",
    "\n",
    "            city_title = city\n",
    "            if city is None:\n",
    "                city_title = 'Nationwide'\n",
    "\n",
    "            #print('There were', total_num_jobs, 'jobs found,', city_title) # Display how many jobs were found\n",
    "\n",
    "            # took out /10 here, which seemed to be assuming always 10 results per page? now 15?\n",
    "            num_pages = int(float(total_num_jobs/10)) # This will be how we know the number of times we need to iterate over each new\n",
    "                                              # search result page\n",
    "\n",
    "            job_descriptions = [] # Store all our descriptions in this list\n",
    "\n",
    "            # used to be 'xrange' here but I changed it to 'range' because I'm using python3 and xrange is python2 only\n",
    "            all_company_results = []\n",
    "            for i in range(1,num_pages+1): # Loop through all of our search result pages\n",
    "                #print('Getting page', i)\n",
    "\n",
    "                start_num = str(i*10) # Assign the multiplier of 10 to view the pages we want\n",
    "                current_page = ''.join([final_site, '&start=', start_num])\n",
    "                # Now that we can view the correct 10 job returns, start collecting the text samples from each\n",
    "\n",
    "                html_page = urlopen(current_page).read() # Get the page\n",
    "\n",
    "                page_obj = BeautifulSoup(html_page,\"lxml\") # Locate all of the job links\n",
    "                job_link_area = page_obj.find(id = 'resultsCol') # The center column on the page where the job postings exist\n",
    "                #print('page_obj',page_obj)  \n",
    "\n",
    "                #print(\"link.get('href')\",base_url + link.get('href')\n",
    "                #print(\"job_link_area.find_all('a')\",job_link_area.find_all('a'))\n",
    "\n",
    "                for a in job_link_area.find_all('a', href=True):\n",
    "                    #print (\"Found the URL:\", a['href'])\n",
    "                    if \"/cmp/\" in a['href']: \n",
    "                        #  if a['href'].find(\"/cmp/\"):\n",
    "                        #print(\"a had /cmp/ in it using find = \",a['href'])\n",
    "                        arrayHrefParts_0 = a['href'].split('/cmp/')\n",
    "                        #print(\"aarrayHrefParts_0 = \",arrayHrefParts_0)\n",
    "                        arrayHrefParts_1 = arrayHrefParts_0[1].split('/')\n",
    "                        #print(\"arrayHrefParts_1 = \",arrayHrefParts_1)\n",
    "                        company = arrayHrefParts_1[0]\n",
    "                        #print('company = ',company)\n",
    "                        all_company_results.append(company)\n",
    "                        #print('all_company_results =',all_company_results)\n",
    "                    else:\n",
    "                        continue\n",
    "                        #print(\"/cmp/ not found in ,\",a['href'])\n",
    "\n",
    "                #print(\"[base_url + link.get('href') for link in job_link_area.find_all('a')]\",[base_url + link.get('href') for link in job_link_area.find_all('a')])\n",
    "\n",
    "                #job_URLS = [base_url + link.get('href') for link in job_link_area.find_all('a')] # Get the URLS for the jobs\n",
    "\n",
    "                #job_URLS = filter(lambda x:'clk' in x, job_URLS) # Now get just the job related URLS\n",
    "\n",
    "\n",
    "                #         for j in xrange(0,len(job_URLS)):\n",
    "                #             final_description = text_cleaner(job_URLS[j])\n",
    "                #             if final_description: # So that we only append when the website was accessed correctly\n",
    "                #                 job_descriptions.append(final_description)\n",
    "                #             sleep(1) # So that we don't be jerks. If you have a very fast internet connection you could hit the server a lot! \n",
    "\n",
    "                sleep(1) # So that we don't be jerks. If you have a very fast internet connection you could hit the server a lot! \n",
    "\n",
    "            for each in range(len(all_company_results)):\n",
    "                if '.html' in all_company_results[each]:\n",
    "                    #print('all_company_results[each]',all_company_results[each],\" each = \",each)\n",
    "                    new_name_wo_html = all_company_results[each].split('.html')[0]\n",
    "                    #print('new_name_wo_html',new_name_wo_html)\n",
    "                    all_company_results[each] = new_name_wo_html  \n",
    "\n",
    "            all_company_results = list(set(all_company_results))\n",
    "            #print ('Done with collecting the job postings!')    \n",
    "            #print ('all_company_results final = ',all_company_results)\n",
    "            #print (\"There were\",len(all_company_results),\" companies successfully found.\")\n",
    "            all_company_results_json = []\n",
    "            for each in all_company_results:\n",
    "                print(\"each in all_company_results = \",each)\n",
    "                print(\"job_title in all_company_results = \",job_title)\n",
    "                temp_obj_holder = {\"company\":\"nothing\",\"job_title\":\"nothing\"}\n",
    "                print(\"temp_obj_holder = \",temp_obj_holder)\n",
    "                temp_obj_holder[\"job_title\"] = job_title\n",
    "                temp_obj_holder[\"company\"] = each\n",
    "                print(\"temp_obj_holder['job_title'] = \",temp_obj_holder[\"job_title\"])\n",
    "                all_company_results_json.append(temp_obj_holder)\n",
    "            return all_company_results_json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The function 'runMultiple_skills_info' runs the 'skills_info' function multiple times, one for each job title in the list given to 'runMultiple_skills_info' function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runMultiple_skills_info(array_of_jobTitles=['puppies']):\n",
    "    all_results = []\n",
    "    for each in array_of_jobTitles:\n",
    "        print('each= ',each)\n",
    "        #temp_results1 = []\n",
    "        temp_results1 = skills_info(\"Houston\",\"TX\",each)\n",
    "        if temp_results1 is None:\n",
    "            continue\n",
    "        else:\n",
    "            print('temp_results is ',temp_results1)\n",
    "            #all_results.extend(temp_results1) \n",
    "            all_results.extend(temp_results1)\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def saveArray_of_JobSearchResults(arrayOfJobsAndSearchTerm, FilePath):\n",
    "    toCSV = arrayOfJobsAndSearchTerm\n",
    "    keys = toCSV[0].keys()\n",
    "    with open(FilePath, 'w') as output_file:\n",
    "        dict_writer = csv.DictWriter(output_file, keys)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(toCSV)\n",
    "    return arrayOfJobsAndSearchTerm\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review:\n",
    "\n",
    "3 functions:\n",
    "- A. skills_info(city=\"Houston\", state=\"TX\",job_title='data+scientist')\n",
    "- B. runMultiple_skills_info(array_of_jobTitles=['puppies'])\n",
    "- C. saveArray_of_JobSearchResults(arrayOfJobsAndSearchTerm = geo_and_geo, FilePath ='../DATA/someFile')\n",
    "\n",
    "A does the web scraping. B does web scraping searching for each of the items in the input array and combines the resuls. C saves the results as a csv file with two columns. One column header is Job Title used in search. The other column is company name looking for that job title in Houston Texas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------\n",
    "# 3. Get addresses and lat longs associated with each company name in json form\n",
    "------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules imported at top of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using <a href=\"https://docs.python.org/3/library/configparser.html\">configparser</a> to handle config file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../config.conf']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configFilePath = '../config.conf'\n",
    "# configparser.ConfigParser().read(configFilePath)\n",
    "config.read(configFilePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### next block sets a variety of useful variables used in the google places API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### sets key as variable that contains the google-places-api key\n",
    "key = config['google-places-api']['key1']\n",
    "#### creates a string used as the prefix to the key in the url used in calling the API service\n",
    "key_pre = '&key='\n",
    "#### the base url in the google places API get call\n",
    "base_url = 'https://maps.googleapis.com/maps/api/place/textsearch/json?query='\n",
    "#### the base url when calling a next page, in other words, when results are more than 20\n",
    "next_page_base_url = 'https://maps.googleapis.com/maps/api/place/textsearch/json?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace some of the characters so the search is a bit better:\n",
    "def cleanCompanies(company_array):\n",
    "    company_array = company_array[1:]\n",
    "    new_array = []\n",
    "    for each in company_array:\n",
    "        job_title = each[1]\n",
    "        each = each[0]\n",
    "        #print(each, \"and type is \",type(each))\n",
    "        print(each.find(\"-\",0))\n",
    "        print(type(each.find(\"-\",0)),\"type of each.find('-',0\")\n",
    "        #print(type(each.find(\"%2C\",1)))\n",
    "        if each.find(\"%2C\",1) is not -1:\n",
    "            each = each.replace(\"%2C\",\",\")\n",
    "            print(each,\"= each\")\n",
    "        else:\n",
    "            each = each\n",
    "        if each.find(\"%26\",1) is not -1:\n",
    "            each = each.replace(\"%26\",\"&\")\n",
    "            #print(each,\"= each find\")\n",
    "        else:\n",
    "            each = each\n",
    "        ##print(each, \"again with each\")\n",
    "        if  each.find(\"-1\",1) is not -1:\n",
    "            each = each.replace(\"-1\",\"\")\n",
    "        #    print(each,\"= each find\")\n",
    "        else:\n",
    "            each = each\n",
    "        if  each.find(\"-2\",0) is not -1:\n",
    "            each = each.replace(\"-2\",\"\")\n",
    "            print(each,\"= each find\")\n",
    "        else: \n",
    "            each = each\n",
    "        if  each.find(\"-3\",0) is not -1:\n",
    "            each = each.replace(\"-3\",\"\")\n",
    "        #    print(each,\"= each find\")\n",
    "        else:\n",
    "            each = each\n",
    "        if  each.find(\"-\",1) is not -1:\n",
    "            each = each.replace(\"-\",\"+\")\n",
    "            print(each,\"= each find\")\n",
    "        else:\n",
    "            each = each\n",
    "        print(\"each at end\",each)\n",
    "        new_array.extend([[each,job_title]])\n",
    "    print('new_array',new_array)\n",
    "    return new_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All the functions that load and clean the company + job title list from csv into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_listCompanyJobtitle(path):\n",
    "    df=pd.read_csv(path,header=None)\n",
    "    company_array = df.values\n",
    "    company_array_formatted = cleanCompanies(company_array)\n",
    "    return company_array_formatted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next few functions run the company names through the google places API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Below is the main function that calls the Google Places API to get the initial results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function that takes a datset that is \n",
    "def callGooglePlacesAPI(search_term,nameOfSearch,job_title):\n",
    "    print('job_title put into googleAPI call is =',job_title)\n",
    "    dataset_I = requests.get(base_url+search_term+key_pre+key)\n",
    "    next_page_result = []\n",
    "    try:\n",
    "        next_page_token = dataset_I.json()['next_page_token']\n",
    "        next_page_result = callNextPageResults(nameOfSearch,next_page_token,job_title)\n",
    "    except: \n",
    "        pass\n",
    "    #####if 'next_page_token' in dataset_I:\n",
    "    #next_page_token = dataset_I.json()['next_page_token']\n",
    "    #print(\"next_page_token\",next_page_token)\n",
    "    #callNextPageResults(nameOfSearch,next_page_token)\n",
    "    #####next_page_results = makeFeatureCollectionsFromPlaces(dataset_I['next_page_token'],nameOfSearch).json()['results']\n",
    "    ##### #else: \n",
    "    ####    next_page = \"none needed\"\n",
    "    dataset = dataset_I.json()['results']\n",
    "    #if 'next_page_token' in dataset_I:\n",
    "    #    dataset.extend(next_page_results)\n",
    "    #    print(\"dataset up top\",dataset)\n",
    "    #else:\n",
    "    #    next_page = \"none needed\"\n",
    "    array_of_features = []\n",
    "    for each in dataset:\n",
    "        photos = []\n",
    "        # centerpoint for coordinates\n",
    "        #print(each)\n",
    "        lat = each['geometry']['location']['lat']\n",
    "        long = each['geometry']['location']['lng']\n",
    "        # properties\n",
    "        address = each['formatted_address']\n",
    "        id = each['id']\n",
    "        name = each['name']\n",
    "        # photos is an array\n",
    "        try:\n",
    "            photos = each['photos']\n",
    "        except:\n",
    "            photos = []\n",
    "            #print(\"photos , attribute error but kept going\")\n",
    "        place_id = each['place_id']\n",
    "        try:\n",
    "            rating = each['rating']\n",
    "        except:\n",
    "            rating = 'NA'\n",
    "            #print(\"rating , attribute error but kept going\")\n",
    "        reference = each['reference']\n",
    "        # types is an array\n",
    "        types = each['types']\n",
    "        testPoint = Point((long, lat))\n",
    "        test2_geoJSOn = Feature(geometry=testPoint, properties={\"job_title\":job_title,\"name\": name,\"address\":address,\"id\":id,\"photos\":photos,\"place_id\":place_id,\"rating\":rating,\"reference\":reference,\"types\":types})\n",
    "        array_of_features.extend([test2_geoJSOn])\n",
    "#     new_FeatureCollection = FeatureCollection(array_of_features) \n",
    "#     with open('../DATA/'+nameOfSearch+'_test.geojson', 'w') as f:\n",
    "#         json.dump(new_FeatureCollection, f)\n",
    "#     return new_FeatureCollection\n",
    "    print(\"array_of_features = \",array_of_features,\" and next_page_result = \",next_page_result)\n",
    "    print(\"types: for \"+search_term+\" type(next_page_result)= \",type(next_page_result),\" and type(array_of_features)=  \",type(array_of_features))\n",
    "    if next_page_result is None or next_page_result==[]:\n",
    "        fakeVariable = 3\n",
    "        #print(\"no second page\")\n",
    "    else:\n",
    "        #print(\"next_page_result passed none test and is \",next_page_result)\n",
    "        array_of_features.extend(next_page_result)\n",
    "        #print(\"array_of_features after extension = \",array_of_features)\n",
    "    #print(\"2nd statement of array_of_features = \",array_of_features)\n",
    "    return array_of_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Below is a secondary function that calls the Google Places API \"next page\" to get any results further additional to the first 40. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def callNextPageResults(nameOfSearch,keyForNextPage,job_title):\n",
    "    time.sleep(5)\n",
    "    #print('next_page_base_url+\"&pagetoken=\"+keyForNextPage+key_pre+key =',next_page_base_url+\"pagetoken=\"+keyForNextPage+key_pre+key)\n",
    "    dataset_I = requests.get(next_page_base_url+\"pagetoken=\"+keyForNextPage+key_pre+key)\n",
    "    #print(\"dataset_I second page full response\",dataset_I.json())\n",
    "    dataset = dataset_I.json()['results']\n",
    "    array_of_features = []\n",
    "    for each in dataset:\n",
    "        photos = []\n",
    "        # centerpoint for coordinates\n",
    "        #print(each)\n",
    "        lat = each['geometry']['location']['lat']\n",
    "        long = each['geometry']['location']['lng']\n",
    "        # properties\n",
    "        address = each['formatted_address']\n",
    "        id = each['id']\n",
    "        name = each['name']\n",
    "        # photos is an array\n",
    "        try:\n",
    "            photos = each['photos']\n",
    "        except:\n",
    "            photos = []\n",
    "            #print(\"photos , attribute error but kept going\")\n",
    "        place_id = each['place_id']\n",
    "        try:\n",
    "            rating = each['rating']\n",
    "        except:\n",
    "            rating = 'NA'\n",
    "            #print(\"rating , attribute error but kept going\")\n",
    "        reference = each['reference']\n",
    "        # types is an array\n",
    "        types = each['types']\n",
    "        testPoint = Point((long, lat))\n",
    "        test2_geoJSOn = Feature(geometry=testPoint, properties={\"job_title\":job_title,\"name\": name,\"address\":address,\"id\":id,\"photos\":photos,\"place_id\":place_id,\"rating\":rating,\"reference\":reference,\"types\":types})\n",
    "        array_of_features.extend([test2_geoJSOn])\n",
    "    #new_FeatureCollection = FeatureCollection(array_of_features) \n",
    "    #with open('../DATA/'+nameOfSearch+'_2ndPage_test.geojson', 'w') as f:\n",
    "    #    json.dump(new_FeatureCollection, f)\n",
    "    #print(\"second page results = \",new_FeatureCollection)\n",
    "    #return new_FeatureCollection\n",
    "    return array_of_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Below is the a function that runs through a list and calls the two functions above and combines the results into a single result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_List_Features(cp_array_formatted):\n",
    "    array_of_all_features = []\n",
    "    print(\"in function create_List_Features, cp_array_formatted[0][0]\",cp_array_formatted[0][0])\n",
    "    for each in cp_array_formatted:\n",
    "        #try:\n",
    "        features_1 = callGooglePlacesAPI(each[1]+\"+Houston+Texas\",\"random_for_now\",each[0])\n",
    "        #print(\"features for a company name =\",features_1)\n",
    "        if array_of_all_features == [] or array_of_all_features is None:\n",
    "            array_of_all_features = features_1\n",
    "        else:\n",
    "            if features_1 is None or features_1 == []:\n",
    "                pass\n",
    "            else:\n",
    "                array_of_all_features.extend(features_1)\n",
    "        #except:\n",
    "        #    print(\"error in except\",error)\n",
    "        \n",
    "        #print(\"array_of_all_features = \",array_of_all_features)\n",
    "    \n",
    "    return array_of_all_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### function that takes a list of features and returns a feature collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creates feature collection from list of features\n",
    "def createFeatureCollection(list_of_features,geographic_limits):\n",
    "    slimmed_featList = []\n",
    "    print('geographic_limits =',geographic_limits)\n",
    "    for each in list_of_features:\n",
    "        print(\"each = \",each)\n",
    "        print(\"each['geometry']['coordinates']\",each['geometry']['coordinates'])\n",
    "        print(\"each['geometry']['coordinates'][0] =\",each['geometry']['coordinates'][0])\n",
    "        if each['geometry']['coordinates'][0] > geographic_limits[\"west_limit\"] and each['geometry']['coordinates'][0] < geographic_limits[\"east_limit\"] and each['geometry']['coordinates'][1] < geographic_limits[\"north_limit\"] and each['geometry']['coordinates'][1] > geographic_limits[\"south_limit\"]:\n",
    "            slimmed_featList.append(each)\n",
    "    \n",
    "    \n",
    "    print('finished the function createFeatureCollection and slimmed featureList is slimmed_featList',slimmed_featList)\n",
    "    return FeatureCollection(slimmed_featList)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### function that takes the return from the above function and saves it as a geoJSON file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 30°15'6.44\"N   96° 5'17.13\"W\n",
    "  29°16'56.13\"N  95°57'46.47\"W\n",
    "   29°21'28.30\"N   94°38'24.37\"W\n",
    "    30°26'55.88\"N   94°48'6.63\"W\n",
    "    \n",
    "    north limit = 30°26'55.88      30.44885556\n",
    "    south limit = 29°16'56.13\"N     29.28225833\n",
    "    west limit = 96° 5'17.13\"W     -96.08805556\n",
    "    east limit = 94°38'24.37\"W     -94.64000000\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert feature collection to geoJSON\n",
    "def convertFeatureCollectionToGeoJSON(featCollection,fileNamePath):\n",
    "    print('got to the function convertFeatureCollectionToGeoJSON')\n",
    "    with open(fileNamePath, 'w') as f:\n",
    "        json.dump(featCollection, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### function that calls the others. It takes as arguments the list of companies and job_titles & the path to save the resulting geojson to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_geojson_from_CompanyJobList(company_list_formatted,path,geographic_limits):\n",
    "    #### function that calls the google api and returns results in terms of a list of feature\n",
    "    print(\"make_geojson_from_CompanyJobList(company_list_formatted = \",company_list_formatted)\n",
    "    list_of_features = create_List_Features(company_list_formatted)\n",
    "    #### function that creates a feature collection from the list of features\n",
    "    featCollection = createFeatureCollection(list_of_features,geographic_limits)\n",
    "    #### function that takes the feature collection and file path and saves the feature collection as a geoJSON file\n",
    "    convertFeatureCollectionToGeoJSON(featCollection,path)\n",
    "    return featCollection\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The geographic limits that trim down the results to only things around Houston as some jobs are advertised in Houston that aren't located in Houston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "geographic_limits = {\"north_limit\":30.454961,\"south_limit\":28.956857,\"west_limit\":-96.206159,\"east_limit\":-94.64000000}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------\n",
    "## Run Everything As Single Function\n",
    "------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getJSON_from_jobsList_everything(array_of_job_Titles,FilePath,path,geographic_limits):\n",
    "    #### returns list of companies advertising each job in an array of job titles\n",
    "    list_of_CompaniesAndJobs = runMultiple_skills_info(array_of_job_Titles)\n",
    "    #### saves the list of lists from above... and formats it a bit further in prep for next step\n",
    "    Array_of_JobCompany_edA = saveArray_of_JobSearchResults(list_of_CompaniesAndJobs, FilePath)\n",
    "    print(\"in geoJSON_from_jobsList_everything, Array_of_JobCompany is =\",Array_of_JobCompany_edA)\n",
    "    ##### opens and cleans the list of companies \n",
    "    Array_of_JobCompany_cln = load_listCompanyJobtitle(FilePath)\n",
    "    #### runs the companies through the google places API\n",
    "    #### combines the results into an array of features\n",
    "    #### turns that into a feature collection \n",
    "    #### then saves as geojson\n",
    "    feature_collection = make_geojson_from_CompanyJobList(Array_of_JobCompany_cln,path,geographic_limits)\n",
    "    #### returns the previously created feature collection for testing of result without reloading from geojson\n",
    "    return feature_collection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running this 3 sets of the job titles list to minimize possible problem of wireless dropping in middle:\n",
    "short_job_title_list_1st3rd\n",
    "short_job_title_list_2nd3rd\n",
    "short_job_title_list_3rd3rd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input_List_JobTitles = short_job_title_list_1st3rd\n",
    "\n",
    "FilePath = '../DATA/test_jobTitleCompany_1of3_v1.csv'\n",
    "\n",
    "path=\"../DATA/test_geojson_Everything_1of3_v1.geojson\"\n",
    "\n",
    "geographic_limits ={\"north_limit\":30.454961,\"south_limit\":28.956857,\"west_limit\":-96.206159,\"east_limit\":-94.64000000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "funcArg = {\"ListJobTitles\":short_job_title_list_1st3rd,\"pCSV\":'../DATA/test_jobTitleCompany_1of3_v1.csv',\"pGeoJSON\":\"../DATA/test_geojson_Everything_1of3_v1.geojson\",\"geo_lim\":{\"north_limit\":30.454961,\"south_limit\":28.956857,\"west_limit\":-96.206159,\"east_limit\":-94.64000000}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "each=  Biochemistry\n",
      "soup.find(id = 'searchCount') =  <div id=\"searchCount\">Jobs 1 to 10 of 40</div>\n",
      "soup.find(id = 'searchCount') =  <class 'bs4.element.Tag'>\n",
      "each in all_company_results =  Baylor-College-of-Medicine\n",
      "job_title in all_company_results =  Biochemistry\n",
      "temp_obj_holder =  {'company': 'nothing', 'job_title': 'nothing'}\n",
      "temp_obj_holder['job_title'] =  Biochemistry\n",
      "each in all_company_results =  Baylor-Medicine\n",
      "job_title in all_company_results =  Biochemistry\n",
      "temp_obj_holder =  {'company': 'nothing', 'job_title': 'nothing'}\n",
      "temp_obj_holder['job_title'] =  Biochemistry\n",
      "each in all_company_results =  University-of-Houston\n",
      "job_title in all_company_results =  Biochemistry\n",
      "temp_obj_holder =  {'company': 'nothing', 'job_title': 'nothing'}\n",
      "temp_obj_holder['job_title'] =  Biochemistry\n",
      "each in all_company_results =  The-Princeton-Review\n",
      "job_title in all_company_results =  Biochemistry\n",
      "temp_obj_holder =  {'company': 'nothing', 'job_title': 'nothing'}\n",
      "temp_obj_holder['job_title'] =  Biochemistry\n",
      "each in all_company_results =  Ip-Recruiter-Group\n",
      "job_title in all_company_results =  Biochemistry\n",
      "temp_obj_holder =  {'company': 'nothing', 'job_title': 'nothing'}\n",
      "temp_obj_holder['job_title'] =  Biochemistry\n",
      "each in all_company_results =  Elite-Medical-Scribes\n",
      "job_title in all_company_results =  Biochemistry\n",
      "temp_obj_holder =  {'company': 'nothing', 'job_title': 'nothing'}\n",
      "temp_obj_holder['job_title'] =  Biochemistry\n",
      "each in all_company_results =  Varsity-Tutors\n",
      "job_title in all_company_results =  Biochemistry\n",
      "temp_obj_holder =  {'company': 'nothing', 'job_title': 'nothing'}\n",
      "temp_obj_holder['job_title'] =  Biochemistry\n",
      "each in all_company_results =  Saudi-Aramco\n",
      "job_title in all_company_results =  Biochemistry\n",
      "temp_obj_holder =  {'company': 'nothing', 'job_title': 'nothing'}\n",
      "temp_obj_holder['job_title'] =  Biochemistry\n",
      "each in all_company_results =  Expertox-3\n",
      "job_title in all_company_results =  Biochemistry\n",
      "temp_obj_holder =  {'company': 'nothing', 'job_title': 'nothing'}\n",
      "temp_obj_holder['job_title'] =  Biochemistry\n",
      "each in all_company_results =  Houston-Methodist\n",
      "job_title in all_company_results =  Biochemistry\n",
      "temp_obj_holder =  {'company': 'nothing', 'job_title': 'nothing'}\n",
      "temp_obj_holder['job_title'] =  Biochemistry\n",
      "each in all_company_results =  Experis\n",
      "job_title in all_company_results =  Biochemistry\n",
      "temp_obj_holder =  {'company': 'nothing', 'job_title': 'nothing'}\n",
      "temp_obj_holder['job_title'] =  Biochemistry\n",
      "each in all_company_results =  Lonestar-College-System-1\n",
      "job_title in all_company_results =  Biochemistry\n",
      "temp_obj_holder =  {'company': 'nothing', 'job_title': 'nothing'}\n",
      "temp_obj_holder['job_title'] =  Biochemistry\n",
      "each in all_company_results =  Rice-University\n",
      "job_title in all_company_results =  Biochemistry\n",
      "temp_obj_holder =  {'company': 'nothing', 'job_title': 'nothing'}\n",
      "temp_obj_holder['job_title'] =  Biochemistry\n",
      "temp_results is  [{'company': 'Baylor-College-of-Medicine', 'job_title': 'Biochemistry'}, {'company': 'Baylor-Medicine', 'job_title': 'Biochemistry'}, {'company': 'University-of-Houston', 'job_title': 'Biochemistry'}, {'company': 'The-Princeton-Review', 'job_title': 'Biochemistry'}, {'company': 'Ip-Recruiter-Group', 'job_title': 'Biochemistry'}, {'company': 'Elite-Medical-Scribes', 'job_title': 'Biochemistry'}, {'company': 'Varsity-Tutors', 'job_title': 'Biochemistry'}, {'company': 'Saudi-Aramco', 'job_title': 'Biochemistry'}, {'company': 'Expertox-3', 'job_title': 'Biochemistry'}, {'company': 'Houston-Methodist', 'job_title': 'Biochemistry'}, {'company': 'Experis', 'job_title': 'Biochemistry'}, {'company': 'Lonestar-College-System-1', 'job_title': 'Biochemistry'}, {'company': 'Rice-University', 'job_title': 'Biochemistry'}]\n",
      "each=  Bioinformatics\n",
      "soup.find(id = 'searchCount') =  <div id=\"searchCount\">Jobs 1 to 10 of 45</div>\n",
      "soup.find(id = 'searchCount') =  <class 'bs4.element.Tag'>\n",
      "each in all_company_results =  Md-Anderson-Cancer-Center\n",
      "job_title in all_company_results =  Bioinformatics\n",
      "temp_obj_holder =  {'company': 'nothing', 'job_title': 'nothing'}\n",
      "temp_obj_holder['job_title'] =  Bioinformatics\n",
      "each in all_company_results =  Baylor-College-of-Medicine\n",
      "job_title in all_company_results =  Bioinformatics\n",
      "temp_obj_holder =  {'company': 'nothing', 'job_title': 'nothing'}\n",
      "temp_obj_holder['job_title'] =  Bioinformatics\n",
      "each in all_company_results =  Burnett-Specialists\n",
      "job_title in all_company_results =  Bioinformatics\n",
      "temp_obj_holder =  {'company': 'nothing', 'job_title': 'nothing'}\n",
      "temp_obj_holder['job_title'] =  Bioinformatics\n",
      "each in all_company_results =  The-University-of-Texas-Health-Science-Center-At-Houston-1\n",
      "job_title in all_company_results =  Bioinformatics\n",
      "temp_obj_holder =  {'company': 'nothing', 'job_title': 'nothing'}\n",
      "temp_obj_holder['job_title'] =  Bioinformatics\n",
      "each in all_company_results =  University-of-Houston\n",
      "job_title in all_company_results =  Bioinformatics\n",
      "temp_obj_holder =  {'company': 'nothing', 'job_title': 'nothing'}\n",
      "temp_obj_holder['job_title'] =  Bioinformatics\n",
      "each in all_company_results =  Human-Genome-Sequencing-Center%2C-Baylor-College-of-Medicine\n",
      "job_title in all_company_results =  Bioinformatics\n",
      "temp_obj_holder =  {'company': 'nothing', 'job_title': 'nothing'}\n",
      "temp_obj_holder['job_title'] =  Bioinformatics\n",
      "each in all_company_results =  Tessella\n",
      "job_title in all_company_results =  Bioinformatics\n",
      "temp_obj_holder =  {'company': 'nothing', 'job_title': 'nothing'}\n",
      "temp_obj_holder['job_title'] =  Bioinformatics\n",
      "each in all_company_results =  Human-Genome-Sequencing-Center,-Baylor-College-of-Medicine\n",
      "job_title in all_company_results =  Bioinformatics\n",
      "temp_obj_holder =  {'company': 'nothing', 'job_title': 'nothing'}\n",
      "temp_obj_holder['job_title'] =  Bioinformatics\n",
      "each in all_company_results =  The-Methodist-Hospital-Research-Institute-%26-Weill-Cornell-Medical-College-of-Cornell-University\n",
      "job_title in all_company_results =  Bioinformatics\n",
      "temp_obj_holder =  {'company': 'nothing', 'job_title': 'nothing'}\n",
      "temp_obj_holder['job_title'] =  Bioinformatics\n",
      "each in all_company_results =  Uthealth-Science-Center-At-Houston\n",
      "job_title in all_company_results =  Bioinformatics\n",
      "temp_obj_holder =  {'company': 'nothing', 'job_title': 'nothing'}\n",
      "temp_obj_holder['job_title'] =  Bioinformatics\n",
      "each in all_company_results =  Ut-Health-Science-Center-At-Houston\n",
      "job_title in all_company_results =  Bioinformatics\n",
      "temp_obj_holder =  {'company': 'nothing', 'job_title': 'nothing'}\n",
      "temp_obj_holder['job_title'] =  Bioinformatics\n",
      "temp_results is  [{'company': 'Md-Anderson-Cancer-Center', 'job_title': 'Bioinformatics'}, {'company': 'Baylor-College-of-Medicine', 'job_title': 'Bioinformatics'}, {'company': 'Burnett-Specialists', 'job_title': 'Bioinformatics'}, {'company': 'The-University-of-Texas-Health-Science-Center-At-Houston-1', 'job_title': 'Bioinformatics'}, {'company': 'University-of-Houston', 'job_title': 'Bioinformatics'}, {'company': 'Human-Genome-Sequencing-Center%2C-Baylor-College-of-Medicine', 'job_title': 'Bioinformatics'}, {'company': 'Tessella', 'job_title': 'Bioinformatics'}, {'company': 'Human-Genome-Sequencing-Center,-Baylor-College-of-Medicine', 'job_title': 'Bioinformatics'}, {'company': 'The-Methodist-Hospital-Research-Institute-%26-Weill-Cornell-Medical-College-of-Cornell-University', 'job_title': 'Bioinformatics'}, {'company': 'Uthealth-Science-Center-At-Houston', 'job_title': 'Bioinformatics'}, {'company': 'Ut-Health-Science-Center-At-Houston', 'job_title': 'Bioinformatics'}]\n",
      "each=  Biologist\n",
      "soup.find(id = 'searchCount') =  <div id=\"searchCount\">Jobs 1 to 8 of 9</div>\n",
      "soup.find(id = 'searchCount') =  <class 'bs4.element.Tag'>\n",
      "temp_results is  []\n",
      "each=  biomechanical\n",
      "soup.find(id = 'searchCount') =  <div id=\"searchCount\">Jobs 1 to 4 of 5</div>\n",
      "soup.find(id = 'searchCount') =  <class 'bs4.element.Tag'>\n",
      "temp_results is  []\n",
      "each=  Biomedical\n",
      "soup.find(id = 'searchCount') =  <div id=\"searchCount\">Jobs 1 to 10 of 146</div>\n",
      "soup.find(id = 'searchCount') =  <class 'bs4.element.Tag'>\n"
     ]
    }
   ],
   "source": [
    "featCollect_1of3  = getJSON_from_jobsList_everything(funcArg[\"ListJobTitles\"],funcArg[\"pCSV\"],funcArg[\"pGeoJSON\"],funcArg[\"geo_lim\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input_List_JobTitles = short_job_title_list_2nd3rd\n",
    "\n",
    "FilePath = '../DATA/test_jobTitleCompany_2of3_v1.csv'\n",
    "\n",
    "path=\"../DATA/test_geojson_Everything_2of3_v1.geojson\"\n",
    "\n",
    "geographic_limits ={\"north_limit\":30.454961,\"south_limit\":28.956857,\"west_limit\":-96.206159,\"east_limit\":-94.64000000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "funcArg = {\"ListJobTitles\":short_job_title_list_2nd3rd,\"pCSV\":'../DATA/test_jobTitleCompany_2of3_v1.csv',\"pGeoJSON\":\"../DATA/test_geojson_Everything_2of3_v1.geojson\",\"geo_lim\":{\"north_limit\":30.454961,\"south_limit\":28.956857,\"west_limit\":-96.206159,\"east_limit\":-94.64000000}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featCollect_2of3  = getJSON_from_jobsList_everything(funcArg[\"ListJobTitles\"],funcArg[\"pCSV\"],funcArg[\"pGeoJSON\"],funcArg[\"geo_lim\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input_List_JobTitles = short_job_title_list_3rd3rd \n",
    "\n",
    "FilePath = '../DATA/test_jobTitleCompany_2of3_v1.csv'\n",
    "\n",
    "path=\"../DATA/test_geojson_Everything_2of3_v1.geojson\"\n",
    "\n",
    "geographic_limits ={\"north_limit\":30.454961,\"south_limit\":28.956857,\"west_limit\":-96.206159,\"east_limit\":-94.64000000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "funcArg = {\"ListJobTitles\":short_job_title_list_3rd3rd,\"pCSV\":'../DATA/test_jobTitleCompany_3of3_v1.csv',\"pGeoJSON\":\"../DATA/test_geojson_Everything_3of3_v1.geojson\",\"geo_lim\":{\"north_limit\":30.454961,\"south_limit\":28.956857,\"west_limit\":-96.206159,\"east_limit\":-94.64000000}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "featCollect_3of3  = getJSON_from_jobsList_everything(funcArg[\"ListJobTitles\"],funcArg[\"pCSV\"],funcArg[\"pGeoJSON\"],funcArg[\"geo_lim\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
